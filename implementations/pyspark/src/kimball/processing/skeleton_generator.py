from delta.tables import DeltaTable
from pyspark.sql import DataFrame, SparkSession
from pyspark.sql.functions import col, current_timestamp, lit


class SkeletonGenerator:
    """
    Handles the generation of 'Skeleton' dimension rows for Early Arriving Facts.
    """

    def __init__(self, spark_session: SparkSession | None = None) -> None:
        if spark_session is None:
            from databricks.sdk.runtime import spark

            spark_session = spark
        self.spark = spark_session

    def generate_skeletons(
        self,
        fact_df: DataFrame,
        dim_table_name: str,
        fact_join_key: str,
        dim_join_key: str,
        surrogate_key_col: str,
        surrogate_key_strategy: str,
    ) -> None:
        """
        Identifies missing keys in the dimension table and inserts skeleton rows.
        """
        if not self.spark.catalog.tableExists(dim_table_name):
            print(
                f"Dimension table {dim_table_name} does not exist. Skipping skeleton generation."
            )
            return

        dim_table = DeltaTable.forName(self.spark, dim_table_name)
        dim_df = dim_table.toDF()

        # 1. Identify Distinct Keys in Fact
        fact_keys = fact_df.select(col(fact_join_key).alias("key")).distinct()

        # 2. Identify Existing Keys in Dimension
        dim_keys = dim_df.select(col(dim_join_key).alias("key")).distinct()

        # 3. Find Missing Keys (Left Anti Join)
        missing_keys = fact_keys.join(dim_keys, "key", "left_anti")

        if missing_keys.isEmpty():
            print(f"No missing keys found for {dim_table_name}.")
            return

        print(f"Found missing keys for {dim_table_name}. Generating skeletons...")

        # 4. Prepare Skeleton Rows
        # We need to match the schema of the dimension table.
        # We'll set the join key, and fill others with defaults/nulls.

        target_schema = dim_df.schema

        # Start with the keys
        skeletons = missing_keys.withColumnRenamed("key", dim_join_key)

        # Add standard SCD2 columns
        skeletons = (
            skeletons.withColumn("__is_current", lit(True))
            .withColumn("__valid_from", lit("1900-01-01 00:00:00").cast("timestamp"))
            .withColumn("__valid_to", lit(None).cast("timestamp"))
            .withColumn("__etl_processed_at", current_timestamp())
            .withColumn("__etl_batch_id", lit("SKELETON_GEN"))
        )

        # Add other columns from schema as NULL (except SK and Join Key)
        # Optimization: Build projection list for missing columns instead of looping withColumn
        # This prevents Catalyst plan explosion for wide tables
        # Also cache columns in a set to avoid repeated JNI calls in the loop
        select_exprs = [col(c) for c in skeletons.columns]
        existing_cols = set(skeletons.columns)

        for field in target_schema.fields:
            if field.name not in existing_cols and field.name != surrogate_key_col:
                select_exprs.append(lit(None).cast(field.dataType).alias(field.name))

        skeletons = skeletons.select(*select_exprs)

        # 5. Generate Surrogate Keys
        if surrogate_key_strategy == "identity":
            # Identity columns are auto-generated on insert. We don't need to generate them.
            # But we must NOT include the column in the DataFrame if it's GENERATED ALWAYS.
            # If it's GENERATED BY DEFAULT, we can.
            # Usually safest to omit it and let Delta handle it.
            pass
        elif surrogate_key_strategy == "hash":
            # Hash the natural key
            # We need to import compute_hashdiff or implement simple hash
            # Let's reuse the strategy logic or just hash the key
            from pyspark.sql.functions import xxhash64

            skeletons = skeletons.withColumn(
                surrogate_key_col, xxhash64(col(dim_join_key).cast("string"))
            )
        elif surrogate_key_strategy == "sequence":
            # Not implemented safely for concurrent runs yet
            pass

        # 6. Insert Skeletons
        # We use append.
        # Note: If using Identity Columns, we must ensure the SK col is NOT in the DF if it's ALWAYS generated.

        cols_to_write = [f.name for f in target_schema.fields]

        if surrogate_key_strategy == "identity":
            if surrogate_key_col in skeletons.columns:
                skeletons = skeletons.drop(surrogate_key_col)
            cols_to_write = [c for c in cols_to_write if c != surrogate_key_col]

        # Select only columns that exist in target (schema evolution might handle others if enabled, but let's be safe)
        final_skeletons = skeletons.select(*cols_to_write)

        final_skeletons.write.format("delta").mode("append").saveAsTable(dim_table_name)
        print(f"Inserted skeleton rows into {dim_table_name}.")
