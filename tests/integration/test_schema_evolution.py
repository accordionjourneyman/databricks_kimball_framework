import os
import tempfile

from pyspark.sql import SparkSession

from kimball.orchestration.orchestrator import Orchestrator


def test_schema_evolution_with_new_column(spark: SparkSession):
    """
    Integration test for schema evolution: Add a new column to source and verify
    SCD2 merge and IDENTITY columns handle it gracefully.
    """
    # Create temporary directories for configs
    with tempfile.TemporaryDirectory() as temp_dir:
        config_path = os.path.join(temp_dir, "test_config.yml")

        # Initial config without new column
        initial_config = """
table_name: test_db.dim_customer
table_type: dimension
scd_type: 2
keys:
  surrogate_key: customer_sk
  natural_keys: [customer_id]
surrogate_key_strategy: identity
track_history_columns: [first_name, last_name]
sources:
  - name: test_db.customers
    alias: c
    cdc_strategy: full
transformation_sql: |
  SELECT
    c.customer_id,
    c.first_name,
    c.last_name
  FROM c
"""

        # Write initial config
        with open(config_path, "w") as f:
            f.write(initial_config)

        # Create source table
        spark.sql("""
        CREATE TABLE test_db.customers (
            customer_id INT,
            first_name STRING,
            last_name STRING
        ) USING DELTA
        """)

        spark.sql("""
        INSERT INTO test_db.customers VALUES
        (1, 'John', 'Doe'),
        (2, 'Jane', 'Smith')
        """)

        # Run initial load
        os.environ["KIMBALL_ETL_SCHEMA"] = "test_db"
        orchestrator = Orchestrator(config_path)
        result = orchestrator.run()
        assert result["status"] == "SUCCESS"

        # Verify table created with IDENTITY using DDL check (more robust than schema string)
        ddl = spark.sql("SHOW CREATE TABLE test_db.dim_customer").collect()[0][0]
        assert (
            "GENERATED BY DEFAULT AS IDENTITY" in ddl
            or "GENERATED ALWAYS AS IDENTITY" in ddl
        )

        # Now add new column to config
        evolved_config = """
table_name: test_db.dim_customer
table_type: dimension
scd_type: 2
keys:
  surrogate_key: customer_sk
  natural_keys: [customer_id]
surrogate_key_strategy: identity
track_history_columns: [first_name, last_name, email]
sources:
  - name: test_db.customers
    alias: c
    cdc_strategy: full
transformation_sql: |
  SELECT
    c.customer_id,
    c.first_name,
    c.last_name,
    c.email
  FROM c
"""

        # Update source table with new column
        spark.sql("""
        ALTER TABLE test_db.customers ADD COLUMN email STRING
        """)

        spark.sql("""
        UPDATE test_db.customers SET email = 'john@example.com' WHERE customer_id = 1
        """)

        spark.sql("""
        UPDATE test_db.customers SET email = 'jane@example.com' WHERE customer_id = 2
        """)

        # Write evolved config
        with open(config_path, "w") as f:
            f.write(evolved_config)

        # Run evolved load
        orchestrator = Orchestrator(config_path)
        result = orchestrator.run()
        assert result["status"] == "SUCCESS"

        # Verify schema evolution: new column added
        evolved_schema = spark.table("test_db.dim_customer").schema
        email_field = next(
            (f for f in evolved_schema.fields if f.name == "email"), None
        )
        assert email_field is not None

        # Verify data integrity: IDENTITY keys preserved
        data = spark.table("test_db.dim_customer").collect()
        sk_values = [row.customer_sk for row in data]
        assert len(set(sk_values)) == len(sk_values)  # No duplicates

        # Clean up
        spark.sql("DROP TABLE test_db.dim_customer")
        spark.sql("DROP TABLE test_db.customers")
