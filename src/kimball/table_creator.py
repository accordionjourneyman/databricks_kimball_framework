import re

from databricks.sdk.runtime import spark


def _is_valid_identifier(name: str) -> bool:
    """Validate that a name is a safe SQL identifier."""
    return bool(re.match(r"^[A-Za-z_][A-Za-z0-9_]*$", name))


def _quote_table_name(table_name: str) -> str:
    """
    Properly quote a multi-part table name for SQL.
    Converts 'schema.table' to '`schema`.`table`' instead of '`schema.table`'.
    """
    parts = table_name.split(".")
    return ".".join(f"`{part}`" for part in parts)


class TableCreator:
    """
    Handles creation of Delta tables with Liquid Clustering support.
    """

    def create_table_with_clustering(
        self,
        table_name: str,
        schema_df,
        config: dict | None = None,
        cluster_by: list[str] | None = None,
        partition_by: list[str] | None = None,
        surrogate_key_col: str | None = None,
        surrogate_key_strategy: str | None = None,
    ):
        """
        Creates a Delta table with optional Liquid Clustering.

        Args:
            table_name: Full table name (catalog.schema.table)
            schema_df: DataFrame with the desired schema
            config: Table configuration from YAML
            cluster_by: Columns for Liquid Clustering (deprecated, use config)
            partition_by: Columns for partitioning (optional, usually not needed with clustering)
            surrogate_key_col: Name of the surrogate key column (if any)
            surrogate_key_strategy: Strategy for SK generation ('identity', 'hash', 'sequence')
        """
        if spark.catalog.tableExists(table_name):
            print(f"Table {table_name} already exists. Skipping creation.")
            return

        # Check config for liquid clustering
        if config and "cluster_by" in config:
            cluster_by = config["cluster_by"]
            partition_by = None  # Don't use partitioning with liquid clustering
            print(f"Using Liquid Clustering from config: {cluster_by}")
        elif cluster_by:
            print(f"Using provided cluster_by: {cluster_by}")

        # Build CREATE TABLE statement
        # We'll use the DataFrame schema to infer column definitions
        columns = []
        for field in schema_df.schema.fields:
            col_name = field.name

            # Check if this is the surrogate key column with identity strategy
            if col_name == surrogate_key_col and surrogate_key_strategy == "identity":
                # Use GENERATED BY DEFAULT AS IDENTITY - allows explicit values for default rows (-1, -2, -3)
                # while auto-generating for normal inserts
                col_def = (
                    f"{col_name} BIGINT GENERATED BY DEFAULT AS IDENTITY (START WITH 1)"
                )
            else:
                col_def = f"{col_name} {field.dataType.simpleString()}"
                if not field.nullable:
                    col_def += " NOT NULL"
            columns.append(col_def)

        columns_sql = ",\n  ".join(columns)

        # Properly quote multi-part table names for Unity Catalog compatibility
        quoted_table_name = _quote_table_name(table_name)

        create_sql = f"""
        CREATE TABLE {quoted_table_name} (
          {columns_sql}
        )
        USING DELTA
        """

        if cluster_by:
            cluster_cols = ", ".join(cluster_by)
            create_sql += f"\nCLUSTER BY ({cluster_cols})"
        elif partition_by:
            partition_cols = ", ".join(partition_by)
            create_sql += f"\nPARTITIONED BY ({partition_cols})"

        # Enable Change Data Feed by default
        create_sql += "\nTBLPROPERTIES ('delta.enableChangeDataFeed' = 'true')"

        if surrogate_key_col and surrogate_key_strategy == "identity":
            print(f"  - Surrogate key '{surrogate_key_col}' using IDENTITY column")
        if cluster_by:
            print(f"  - Liquid Clustering on {cluster_by}")
        spark.sql(create_sql)
        print(f"Table {table_name} created successfully.")

        # Enable Delta optimizations after table creation (optional features, may fail on Free Edition)
        try:
            self.enable_predictive_optimization(table_name)
        except Exception:
            pass  # Feature not available on this cluster tier

        try:
            self.enable_deletion_vectors(table_name)
        except Exception:
            pass  # Feature not available on this cluster tier

        # Apply basic Delta constraints after table creation
        self.apply_basic_constraints(table_name, surrogate_key_col, schema_df)

        # Apply additional constraints from config
        if config:
            self.apply_delta_constraints(table_name, config)

    def apply_basic_constraints(
        self, table_name: str, surrogate_key_col: str | None = None, schema_df=None
    ):
        """
        Apply basic Delta constraints using ALTER TABLE statements.

        Args:
            table_name: Full table name
            surrogate_key_col: Name of the surrogate key column
            schema_df: DataFrame with schema information
        """
        # Apply surrogate key NOT NULL constraint
        quoted_table_name = _quote_table_name(table_name)
        if surrogate_key_col:
            alter_sql = f"ALTER TABLE {quoted_table_name} ADD CONSTRAINT sk_not_null CHECK (`{surrogate_key_col}` IS NOT NULL)"
            try:
                spark.sql(alter_sql)
                print("Applied surrogate key NOT NULL constraint")
            except Exception as e:
                print(f"Warning: Could not apply surrogate key constraint: {e}")

        # Apply is_current boolean constraint for SCD2 tables
        if schema_df and "__is_current" in [f.name for f in schema_df.schema.fields]:
            alter_sql = f"ALTER TABLE {quoted_table_name} ADD CONSTRAINT is_current_check CHECK (__is_current IN (true, false))"
            try:
                spark.sql(alter_sql)
                print("Applied is_current boolean constraint")
            except Exception as e:
                print(f"Warning: Could not apply is_current constraint: {e}")

    def apply_delta_constraints(self, table_name: str, config: dict):
        """
        Apply Delta constraints based on YAML configuration.

        Args:
            table_name: Full table name
            config: Table configuration from YAML
        """
        # Apply NOT NULL constraints for natural keys
        quoted_table_name = _quote_table_name(table_name)
        natural_keys = config.get("natural_keys", [])
        for key in natural_keys:
            alter_sql = (
                f"ALTER TABLE {quoted_table_name} ALTER COLUMN `{key}` SET NOT NULL"
            )
            try:
                spark.sql(alter_sql)
                print(f"Applied NOT NULL constraint to {key}")
            except Exception as e:
                print(f"Failed to apply NOT NULL to {key}: {e}")

        # Apply business domain constraints
        constraints = config.get("constraints", [])
        for constraint in constraints:
            constraint_name = constraint.get("name")
            constraint_expr = constraint.get("expression")
            if constraint_name and constraint_expr:
                alter_sql = f"ALTER TABLE {quoted_table_name} ADD CONSTRAINT `{constraint_name}` CHECK ({constraint_expr})"
                try:
                    spark.sql(alter_sql)
                    print(f"Applied constraint {constraint_name}")
                except Exception as e:
                    print(f"Failed to apply constraint {constraint_name}: {e}")

    def enable_schema_auto_merge(self):
        """
        Enable schema auto-merge for the current session.
        """
        spark.conf.set("spark.databricks.delta.schema.autoMerge.enabled", "true")
        print("Enabled schema auto-merge for current session")

    def enable_predictive_optimization(self, table_name: str):
        """
        Enables Predictive Optimization for a Delta table.
        This allows Databricks to automatically optimize table layout and performance.
        """
        quoted_table_name = _quote_table_name(table_name)
        alter_sql = f"ALTER TABLE {quoted_table_name} SET TBLPROPERTIES ('delta.enablePredictiveOptimization' = 'true')"
        spark.sql(alter_sql)
        print(f"Predictive Optimization enabled for {table_name}")

    def enable_deletion_vectors(self, table_name: str):
        """
        Enables Deletion Vectors for a Delta table.
        This improves performance for MERGE operations with many updates/deletes.
        """
        quoted_table_name = _quote_table_name(table_name)
        alter_sql = f"ALTER TABLE {quoted_table_name} SET TBLPROPERTIES ('delta.enableDeletionVectors' = 'true')"
        spark.sql(alter_sql)
        print(f"Deletion Vectors enabled for {table_name}")
